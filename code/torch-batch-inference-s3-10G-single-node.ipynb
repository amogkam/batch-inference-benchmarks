{"cells":[{"cell_type":"code","source":["import time\nimport torch\ntorch.__version__\ntorch.cuda.is_available()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"701d2809-daab-4de6-979d-ff8c6aab47cf","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[1]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[1]: True"]}}],"execution_count":0},{"cell_type":"code","source":["print(\"Executor memory: \", spark.conf.get(\"spark.executor.memory\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"59841386-1fb4-4a0c-afbe-922f8f8049d7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Executor memory:  199584m\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Executor memory:  199584m\n"]}}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\nfrom torchvision import transforms\nfrom torchvision.models import resnet50, ResNet50_Weights"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9d5cb47e-49c5-4904-9df3-538972665541","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":{"text/plain":"","application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"},"removedWidgets":[],"addedWidgets":{},"metadata":{"kernelSessionId":"8a06c5a7-c10a247bdebf13efb742f05e"},"type":"mimeBundle","arguments":{}}},"output_type":"display_data","data":{"text/plain":"","application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"}}],"execution_count":0},{"cell_type":"code","source":["# Enable Arrow support.\nspark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a4832578-6320-43f6-a462-bd8eb1ae454b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Create and broadcast model state to all executors\nmodel_state = resnet50(weights=ResNet50_Weights.DEFAULT).state_dict()\nbc_model_state = sc.broadcast(model_state)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b6229f4e-80ba-40ea-bac7-9d07bc2b84fe","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":{"text/plain":"  0%|          | 0.00/97.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b6dc61dd9aa4d089cae896e5b644085"}},"removedWidgets":[],"addedWidgets":{},"metadata":{"kernelSessionId":"8a06c5a7-c10a247bdebf13efb742f05e"},"type":"mimeBundle","arguments":{}}},"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/97.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b6dc61dd9aa4d089cae896e5b644085"}}}],"execution_count":0},{"cell_type":"code","source":["input_data = spark.read.format(\"parquet\").load(\"s3://air-example-data-2/10G-image-data-synthetic-raw-parquet\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"525bb7ea-f7b3-44ff-a013-3d8d9fc82135","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Preprocessing\nfrom pyspark.sql.functions import col, pandas_udf\nfrom pyspark.sql.types import ArrayType, FloatType\n\nimport numpy as np\n\nimport torch\nimport time\n\n# Read documentation here: https://spark.apache.org/docs/3.0.1/sql-pyspark-pandas-with-arrow.html\n\n@pandas_udf(ArrayType(FloatType()))\ndef preprocess(image: pd.Series) -> pd.Series:\n    preprocess = transforms.Compose(\n        [\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ]\n    )\n    print(f\"number of images: {len(image)}\")\n    # Spark has no tensor support, so it flattens the image tensor to a single array during read.\n    # Each image is represented as a flattened numpy array.\n    # We have to reshape back to the original number of dimensions.\n    # Need to convert to float dtype otherwise torchvision transforms will complain. The data is read as short (int16) by default\n    batch_dim = len(image)\n    numpy_batch = np.stack(image.values)\n    reshaped_images = numpy_batch.reshape(batch_dim, 256, 256, 3).astype(np.float)\n    \n    torch_tensor = torch.Tensor(reshaped_images.transpose(0, 3, 1, 2))\n    preprocessed_images = preprocess(torch_tensor).numpy()\n    # Arrow only works with single dimension numpy arrays, so need to flatten the array before outputting it\n    preprocessed_images = [image.flatten() for image in preprocessed_images]\n    return pd.Series(preprocessed_images)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ef059a6f-8c22-44ef-960c-518956975eb2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["preprocessed_data = input_data.select(preprocess(col(\"image\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9af2d6fc-4fb8-4983-aca3-672381e96310","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# 1000 is the largest batch size that can fit on GPU. Limit batch size to 1000 to avoid CUDA OOM.\nspark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1000\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"370ef5d5-55a1-4821-b428-414a62a36620","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@pandas_udf(ArrayType(FloatType()))\ndef predict(preprocessed_images: pd.Series) -> pd.Series:\n    with torch.inference_mode():\n        model = resnet50()\n        model.load_state_dict(bc_model_state.value)\n        model = model.to(torch.device(\"cuda\")) # Move model to GPU\n        model.eval()\n        \n        batch = preprocessed_images\n        batch_dim = len(batch)\n        numpy_batch = np.stack(batch.values)\n        # Spark has no tensor support, so it flattens the image tensor to a single array during read.\n        # Each image is represented as a flattened numpy array.\n        # We have to reshape back to the original number of dimensions.\n        reshaped_images = numpy_batch.reshape(batch_dim, 3, 224, 224)\n        gpu_batch = torch.Tensor(reshaped_images).to(torch.device(\"cuda\"))\n        predictions = list(model(gpu_batch).cpu().numpy())\n        assert len(predictions) == batch_dim\n        \n        return pd.Series(predictions)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"48c15069-1c2f-47c3-9b1c-6fe0eb5641e2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Repartition to 1 to limit parallelism for GPU prediction.\n# Single node clusters do not have GPU scheduling support.\n# Otherwise 32 tasks will run in parallel causing Cuda OOM.\none_partition_data = preprocessed_data.repartition(1)\npredictions = one_partition_data.select(predict(col(\"preprocess(image)\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d30eb582-afe4-433c-9fee-9dc5c6f3cf86","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["start_time = time.time()\npredictions.write.mode(\"overwrite\").format(\"noop\").save()\nend_time = time.time()\nprint(f\"Prediction took: {end_time-start_time} seconds\")\n\nassert preprocessed_data.count() == 16232"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"505599b5-fdd0-4f55-bb9d-f17898858f47","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Prediction took: 137.95954656600952 seconds\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Prediction took: 137.95954656600952 seconds\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"torch-batch-inference-s3-10G-single-node","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":566047737056949}},"nbformat":4,"nbformat_minor":0}
