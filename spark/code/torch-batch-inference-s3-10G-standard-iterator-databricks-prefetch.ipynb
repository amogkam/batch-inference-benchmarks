{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e74a8d6d-e81d-4e60-a239-d03eee92e924",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_data = spark.read.format(\"parquet\").load(\"s3://air-example-data-2/10G-image-data-synthetic-raw-parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "701d2809-daab-4de6-979d-ff8c6aab47cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "torch.__version__\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59841386-1fb4-4a0c-afbe-922f8f8049d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor memory:  148728m\n"
     ]
    }
   ],
   "source": [
    "print(\"Executor memory: \", spark.conf.get(\"spark.executor.memory\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d5cb47e-49c5-4904-9df3-538972665541",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4832578-6320-43f6-a462-bd8eb1ae454b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enable Arrow support.\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6229f4e-80ba-40ea-bac7-9d07bc2b84fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b9a7842b4041a3b66ebd1cd81b60cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and broadcast model state. Equivalent to AIR Checkpoint\n",
    "model_state = resnet50(weights=ResNet50_Weights.DEFAULT).state_dict()\n",
    "# sc is already initialized by Databricks. Broadcast the model state to all executors.\n",
    "bc_model_state = sc.broadcast(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef059a6f-8c22-44ef-960c-518956975eb2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from typing import Iterator\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "preprocess_image = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Read documentation here: https://spark.apache.org/docs/3.0.1/sql-pyspark-pandas-with-arrow.html\n",
    "\n",
    "\n",
    "def preprocess(image_batch):\n",
    "    image_batch = image_batch.reshape(-1, 256, 256, 3)\n",
    "    image_batch = torch.permute(image_batch, (0, 3, 1, 2))\n",
    "    return preprocess_image(image_batch)\n",
    "\n",
    "\n",
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def resnet_predict(pandas_series_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    with torch.inference_mode():\n",
    "        model = resnet50()\n",
    "        model.load_state_dict(bc_model_state.value)\n",
    "        model = model.to(torch.device(\"cuda\")) # Move model to GPU\n",
    "        model.eval()\n",
    "\n",
    "        for pandas_series in pandas_series_iter:\n",
    "            image_batch = torch.tensor(np.stack(pandas_series.values).astype(np.uint8))\n",
    "            # change uint 0 ~ 255 range values to 0 ~ 1 range float32 values\n",
    "            image_batch = image_batch / np.float32(256)\n",
    "\n",
    "            image_batch = preprocess(image_batch)\n",
    "            \n",
    "            \n",
    "            image_batch = image_batch.to(torch.device(\"cuda\"))\n",
    "            \n",
    "            predictions = list(model(image_batch).cpu().numpy())\n",
    "            \n",
    "            yield pd.Series(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9af2d6fc-4fb8-4983-aca3-672381e96310",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = input_data.select(resnet_predict(col(\"image\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "370ef5d5-55a1-4821-b428-414a62a36620",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "505599b5-fdd0-4f55-bb9d-f17898858f47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "run_times = []\n",
    "run_N = 6\n",
    "for i in range(run_N):\n",
    "    start_time = time.time()\n",
    "    predictions.write.mode(\"overwrite\").format(\"noop\").save()\n",
    "    end_time = time.time()\n",
    "    run_times.append(end_time-start_time)\n",
    "\n",
    "assert input_data.count() == 16232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3e66381-0471-40e8-939b-a8de901ec781",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run times:  [126.96703696250916, 106.93883275985718, 99.0119571685791, 95.7514579296112, 90.47398567199707, 90.08183360099792]\nAverge Prediction took: 101.53751734892528 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Run times: \", run_times)\n",
    "print(f\"Averge Prediction took: {sum(run_times) / run_N} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4436de79-835a-4fb9-acc1-8630768561ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=2200977071475194#setting/sparkui/0502-000621-nveaec0v/driver-6759163737913947912\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.146.232.191:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5793f9c370>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e42afc3-a845-4bf2-a6ac-e0fba290dbcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2878901766425688,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(databricks modified) torch-batch-inference-s3-10G-standard-iterator",
   "notebookOrigID": 2252021368504006,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
